<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter2_action/hybrid_tutorial.py" target="_blank">View code on GitHub</a><br><br>PyTorch tutorial of <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">Proximal Policy Optimization (PPO)</span>  algorithm for hybrid action.<br><a href="https://arxiv.org/pdf/1707.06347.pdf">Related Link</a><br><br>PPO is one of the most popular policy gradient methods for deep reinforcement learning. It combines the classic Actor-Critic paradigm and the trust region policy optimization method into a simple yet effect algorithm design. Compared to some traditional RL algorithms like REINFORCE and A2C, PPO can deploy more stable and efficient policy optimization by using clipped surrogate objective mentioned below:<br>$$J(\theta) = \min(\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_k}(a_{t}|s_{t})}A^{\theta_k}(s_{t},a_{t}),\text{clip}(\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_k}(a_{t}|s_{t})}, 1-\epsilon,1+\epsilon)A^{\theta_k}(s_{t},a_{t}))$$<br>The final objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective, which only ignore the change in probability ratio when it would make the objective improve, and we include it when it makes the objective worse.<br>Detailed notation definition can be found in <a href="https://github.com/opendilab/PPOxFamily/blob/main/chapter1_overview/chapter1_notation.pdf">Related Link</a>.<br><br>Hybrid action space, one of the most commonly used action spaces, is often used in practical decision applications such as StarCraftII and Honor of Kings. It contains serveral controllable varaibles and can be formulated into a tree structure. The middle nodes of these trees should be discrete selection and the leaf nodes can be both discrete and continuous actions. Due to this complexity, hybrid action space needs more special algorithm design and code implementation.<br><br>This tutorial is mainly composed of the following three parts with utilities including mask and treetensor, you can learn from these demo codes step by step or using them as code segment in your own program:<br>  - Policy Network Architecture<br>  - Sample Action Function<br>  - Main (Test) Function<br>More visulization results about PPO in hybrid action space can be found in <a href="https://github.com/opendilab/PPOxFamily/issues/4">Related Link</a>.<br><br>P.S, It you need to install treetensor, you can use this command<br>``pip install DI-treetensor``</div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The definition of hybrid action policy network used in PPO, which is mainly composed of three parts: encoder, action_type head (discrete) and action_args head (continuous).</p></div><div class="code"><pre><code id="code_1" name="py_code">from typing import Dict
import torch
import torch.nn as nn
import treetensor.torch as ttorch
from torch.distributions import Normal, Independent


class HybridPolicyNetwork(nn.Module):
    def __init__(self, obs_shape: int, action_shape: Dict[str, int]) -> None:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>        PyTorch necessary requirements for extending <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> . Our network should also subclass this class.</p></div><div class="code"><pre><code id="code_3" name="py_code">        super(HybridPolicyNetwork, self).__init__()</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>        Define encoder module, which maps raw state into embedding vector.<br>        It could be different for various state, such as Convolution Neural Network (CNN) for image state and Multilayer perceptron (MLP) for vector state, respectively.<br>        $$ y = max(W_2 max(W_1x+b_1, 0) + b_2, 0)$$</p></div><div class="code"><pre><code id="code_4" name="py_code">        self.encoder = nn.Sequential(
            nn.Linear(obs_shape, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
        )</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>        Define action_type head module, which outputs discrete logit.<br>        $$ y = Wx + b $$</p></div><div class="code"><pre><code id="code_5" name="py_code">        self.action_type_shape = action_shape['action_type_shape']
        self.action_type_head = nn.Linear(32, self.action_type_shape)</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>        Define action_args head module, which outputs corresponding continuous action arguments.<br>        $$ \mu = Wx + b $$<br>        $$\sigma = e^w$$</p></div><div class="code"><pre><code id="code_6" name="py_code">        self.action_args_shape = action_shape['action_args_shape']
        self.action_args_mu = nn.Linear(32, self.action_args_shape)
        self.action_args_log_sigma = nn.Parameter(torch.zeros(1, self.action_args_shape))
</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The computation graph of hybrid action policy network used in PPO.<br>            <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">x -> encoder -> action_type_head -> action_type_logit</span> .<br>            <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">x -> encoder -> action_args_mu -> \mu</span> .<br>            <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">action_args_log_sigma -> exp -> sigma</span> .</p></div><div class="code"><pre><code id="code_7" name="py_code">    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>        Transform original state into embedding vector, i.e. $$(B, *) -> (B, N)$$</p></div><div class="code"><pre><code id="code_9" name="py_code">        x = self.encoder(x)</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>        Output discrete action logit.</p></div><div class="code"><pre><code id="code_10" name="py_code">        logit = self.action_type_head(x)</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>        Output the argument mu depending on the embedding vector.</p></div><div class="code"><pre><code id="code_11" name="py_code">        mu = self.action_args_mu(x)</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>        Utilize broadcast mechanism to make the same shape between log_sigma and mu.<br>        <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">zeros_like</span> operation doesn't pass gradient.<br>        <a href="https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html#in-brief-tensor-broadcasting">Related Link</a></p></div><div class="code"><pre><code id="code_12" name="py_code">        log_sigma = self.action_args_log_sigma + torch.zeros_like(mu)</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>        Utilize exponential operation to produce the actual sigma.<br>        $$\sigma = e^w$$</p></div><div class="code"><pre><code id="code_13" name="py_code">        sigma = torch.exp(log_sigma)</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>        Return treetensor-type output.</p></div><div class="code"><pre><code id="code_14" name="py_code">        return ttorch.as_tensor({
            'action_type': logit,
            'action_args': {
                'mu': mu,
                'sigma': sigma
            }
        })

</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of sampling hybrid action, input is a treetensor with two keys <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">action_type</span> and <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">action_args</span> .</p></div><div class="code"><pre><code id="code_15" name="py_code">def sample_hybrid_action(logit: ttorch.Tensor) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    Transform logit (raw output of discrete policy head, e.g. last fully connected layer) into probability.<br>    $$\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$$</p></div><div class="code"><pre><code id="code_17" name="py_code">    prob = torch.softmax(logit.action_type, dim=-1)</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    Construct categorical distribution. The probability mass function is: $$f(x=i|\boldsymbol{p})=p_i$$<br>    <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Related Link</a></p></div><div class="code"><pre><code id="code_18" name="py_code">    discrete_dist = torch.distributions.Categorical(probs=prob)</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    Sample one discrete action type per sample (state input).</p></div><div class="code"><pre><code id="code_19" name="py_code">    action_type = discrete_dist.sample()
</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>    Construct gaussian distribution with $$\mu, \sigma$$<br>    $$X \sim \mathcal{N}(\mu,\,\sigma^{2})$$<br>    Its probability density function is: $$f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left( -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{\!2}\,\right)$$<br>    <a href="https://en.wikipedia.org/wiki/Normal_distribution">Related Link</a></p></div><div class="code"><pre><code id="code_20" name="py_code">    continuous_dist = Normal(logit.action_args.mu, logit.action_args.sigma)</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    Reinterpret <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">action_shape</span> gaussian distribution into a multivariate gaussian distribution with<br>    diagonal convariance matrix.<br>    Ensure each event is independent with each other.<br>    <a href="https://pytorch.org/docs/stable/distributions.html#independent">Related Link</a></p></div><div class="code"><pre><code id="code_21" name="py_code">    continuous_dist = Independent(continuous_dist, 1)</code></pre></div></div><div class="section" id="section-22"><div class="docs doc-strings"><p>    Sample one action args of the shape <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">action_shape</span> per sample (state input).</p></div><div class="code"><pre><code id="code_22" name="py_code">    action_args = continuous_dist.sample()</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p>    Return the final parameterized action.</p></div><div class="code"><pre><code id="code_23" name="py_code">    return ttorch.as_tensor({
        'action_type': action_type,
        'action_args': action_args,
    })

</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of testing sampling hybrid action. Construct a hybrid action (parameterized action)<br>        policy and sample a group of action.</p></div><div class="code"><pre><code id="code_24" name="py_code">def test_sample_hybrid_action():</code></pre></div></div><div class="section" id="section-26"><div class="docs doc-strings"><p>    Set batch_size = 4, obs_shape = 10, action_shape is a dict, including 3 possible discrete action types and 3 corresponding continuous arguments. The relationship between action_type and action_args are represented by the below <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">mask</span> .</p></div><div class="code"><pre><code id="code_26" name="py_code">    B, obs_shape, action_shape = 4, 10, {'action_type_shape': 3, 'action_args_shape': 3}
    mask = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]</code></pre></div></div><div class="section" id="section-27"><div class="docs doc-strings"><p>    Generate state data from uniform distribution in [0, 1].</p></div><div class="code"><pre><code id="code_27" name="py_code">    state = torch.rand(B, obs_shape)</code></pre></div></div><div class="section" id="section-28"><div class="docs doc-strings"><p>    Define hybrid action network with encoder, discrete head and continuous head.</p></div><div class="code"><pre><code id="code_28" name="py_code">    policy_network = HybridPolicyNetwork(obs_shape, action_shape)</code></pre></div></div><div class="section" id="section-29"><div class="docs doc-strings"><p>    Policy network forward procedure, input state and output dict-type logit.</p></div><div class="code"><pre><code id="code_29" name="py_code">    logit = policy_network(state)
    assert isinstance(logit, ttorch.Tensor)
    assert logit.action_type.shape == (B, action_shape['action_type_shape'])
    assert logit.action_args.mu.shape == (B, action_shape['action_args_shape'])
    assert logit.action_args.sigma.shape == (B, action_shape['action_args_shape'])</code></pre></div></div><div class="section" id="section-30"><div class="docs doc-strings"><p>    Sample action accoding to corresponding logit part.</p></div><div class="code"><pre><code id="code_30" name="py_code">    action = sample_hybrid_action(logit)
    assert action.action_type.shape == (B, )
    assert action.action_args.shape == (B, action_shape['action_args_shape'])</code></pre></div></div><div class="section" id="section-31"><div class="docs doc-strings"><p>    Acquire each sample's mask by looking up in <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">mask</span> with action typeã€‚</p></div><div class="code"><pre><code id="code_31" name="py_code">    data_mask = torch.as_tensor([mask[i] for i in action.action_type]).bool()</code></pre></div></div><div class="section" id="section-32"><div class="docs doc-strings"><p>    Filter corresponding action_args according to mask and re-assign it.</p></div><div class="code"><pre><code id="code_32" name="py_code">    filtered_action_args = ttorch.masked_select(action.action_args, data_mask)
    action.action_args = filtered_action_args
    assert action.action_args.shape == (B, )</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p>    Select some samples with slicing (for example).</p></div><div class="code"><pre><code id="code_33" name="py_code">    selected_action = action[1:3]
    assert selected_action.action_type.shape == (2, )

</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>