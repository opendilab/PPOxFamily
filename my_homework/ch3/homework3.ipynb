{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码实践题目"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 题目1（奇偶数预测问题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OddAndEven(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # directly input data\n",
    "        self.net_direct = nn.Sequential(\n",
    "            nn.Linear(10, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "        # turn input to binary format，binary encoding length = 9\n",
    "        self.net_binary = nn.Sequential(\n",
    "            nn.Linear(10, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "        # use trigonometric function to encode，length = 9\n",
    "        self.net_trigo = nn.Sequential(\n",
    "            nn.Linear(10, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def _binary_encode(self, x):\n",
    "        \n",
    "        def num2binary(x):\n",
    "            if x == 0:\n",
    "                return []\n",
    "            else:\n",
    "                temp = x % 2\n",
    "                l = num2binary(x//2)\n",
    "                l.append(temp)\n",
    "                return l\n",
    "\n",
    "        binary_encode = []\n",
    "        for i in range(x.size(0)):\n",
    "            sub_1 = num2binary(x[i])\n",
    "            while len(sub_1) < 10: sub_1.insert(0, 0)\n",
    "            binary_encode.append(sub_1)\n",
    "\n",
    "        return torch.tensor(binary_encode, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    def _trigo_encode(self, x):\n",
    "        # div_term = torch.exp(torch.arange(0, 10, 2) *\n",
    "        #                      -(math.log(10000.0) / 10)).to(x.device)\n",
    "        trigo_encode = torch.zeros(x.size(0), 10).to(x.device)\n",
    "        # trigo_encode[..., 0::2] = torch.sin(x * div_term)\n",
    "        # trigo_encode[..., 1::2] = torch.cos(x * div_term)\n",
    "        trigo_encode[..., 0::2] = torch.sin(x).repeat(1, 5)\n",
    "        trigo_encode[..., 1::2] = torch.cos(x).repeat(1, 5)\n",
    "\n",
    "\n",
    "        return trigo_encode.to(x.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        binary_encode = self._binary_encode(x).to(x.device)\n",
    "        trigo_encode = self._trigo_encode(x).to(x.device)\n",
    "        origin_encode = x.repeat(1, 10).to(x.device)\n",
    "\n",
    "        # binary_result = torch.argmax(self.sigmoid(self.net_direct(binary_encode)), dim=-1)\n",
    "        # trigo_result = torch.argmax(self.sigmoid(self.net_direct(trigo_encode)), dim=-1)\n",
    "        # origin_result = torch.argmax(self.sigmoid(self.net_direct(origin_encode)), dim=-1)\n",
    "        binary_result = self.net_binary(binary_encode)\n",
    "        trigo_result = self.net_trigo(trigo_encode)\n",
    "        origin_result = self.net_direct(origin_encode)\n",
    "\n",
    "        return binary_result, trigo_result, origin_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 64\n",
    "    epochs = 200\n",
    "    net = OddAndEven()\n",
    "    net.to(device='cuda')\n",
    "    net.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=1e-3)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        data = torch.randint(0, 1000, size=(batch_size, 1), dtype=torch.float).cuda()\n",
    "        label = (data % 2 == 0).long().cuda().squeeze()\n",
    "        b, t, o = net(data)\n",
    "        l1 = loss_fn(b, label)\n",
    "        l2 = loss_fn(t, label)\n",
    "        l3 = loss_fn(o, label)\n",
    "        \n",
    "        l1.backward()\n",
    "        l2.backward()\n",
    "        l3.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"epoch:[{epoch:>3d}/{epochs:>3d}] binary_loss:{l1} triangle_loss:{l2} origin_loss:{l3}\")\n",
    "    \n",
    "    print('----------------------------- Test -----------------------------' )\n",
    "    data = torch.randint(0, 10, size=(10, 1), dtype=torch.float).cuda()\n",
    "    label = (data % 2 == 0).long().cuda().squeeze()\n",
    "    b, t, o = net(data)\n",
    "    print('binary_result:   ', torch.argmax(b, dim=-1))\n",
    "    print('triangle_result: ', torch.argmax(t, dim=-1))\n",
    "    print('origin_result:   ', torch.argmax(o, dim=-1))\n",
    "    print('label:           ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\73106\\AppData\\Local\\Temp\\ipykernel_51568\\3798698966.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  l = num2binary(x//2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[  0/200] binary_loss:0.7349872589111328 triangle_loss:0.6947663426399231 origin_loss:52.24180221557617\n",
      "epoch:[  1/200] binary_loss:0.683316171169281 triangle_loss:0.6998624801635742 origin_loss:28.34395408630371\n",
      "epoch:[  2/200] binary_loss:0.6706985235214233 triangle_loss:0.6971087455749512 origin_loss:9.654135704040527\n",
      "epoch:[  3/200] binary_loss:0.6902305483818054 triangle_loss:0.7240961790084839 origin_loss:13.732337951660156\n",
      "epoch:[  4/200] binary_loss:0.6652613878250122 triangle_loss:0.7089925408363342 origin_loss:22.04283905029297\n",
      "epoch:[  5/200] binary_loss:0.6852871775627136 triangle_loss:0.6977459788322449 origin_loss:23.781091690063477\n",
      "epoch:[  6/200] binary_loss:0.6469615697860718 triangle_loss:0.7086113691329956 origin_loss:18.387901306152344\n",
      "epoch:[  7/200] binary_loss:0.6761390566825867 triangle_loss:0.6878255009651184 origin_loss:11.120370864868164\n",
      "epoch:[  8/200] binary_loss:0.6498953700065613 triangle_loss:0.6898926496505737 origin_loss:3.8951528072357178\n",
      "epoch:[  9/200] binary_loss:0.6452895998954773 triangle_loss:0.6870980858802795 origin_loss:7.197264671325684\n",
      "epoch:[ 10/200] binary_loss:0.6411842107772827 triangle_loss:0.6955468058586121 origin_loss:15.669432640075684\n",
      "epoch:[ 11/200] binary_loss:0.6168311834335327 triangle_loss:0.691303014755249 origin_loss:17.784526824951172\n",
      "epoch:[ 12/200] binary_loss:0.6193327903747559 triangle_loss:0.6875953674316406 origin_loss:15.500490188598633\n",
      "epoch:[ 13/200] binary_loss:0.6365929841995239 triangle_loss:0.7006270885467529 origin_loss:15.674912452697754\n",
      "epoch:[ 14/200] binary_loss:0.6062679886817932 triangle_loss:0.6948075294494629 origin_loss:5.189105987548828\n",
      "epoch:[ 15/200] binary_loss:0.6067568063735962 triangle_loss:0.6975786685943604 origin_loss:3.1493356227874756\n",
      "epoch:[ 16/200] binary_loss:0.608468770980835 triangle_loss:0.6939046382904053 origin_loss:7.675893306732178\n",
      "epoch:[ 17/200] binary_loss:0.6114965081214905 triangle_loss:0.6975736021995544 origin_loss:8.78271198272705\n",
      "epoch:[ 18/200] binary_loss:0.6031761169433594 triangle_loss:0.6943066716194153 origin_loss:8.025813102722168\n",
      "epoch:[ 19/200] binary_loss:0.5792003273963928 triangle_loss:0.6954648494720459 origin_loss:6.359289169311523\n",
      "epoch:[ 20/200] binary_loss:0.5873932838439941 triangle_loss:0.6948984265327454 origin_loss:1.4420090913772583\n",
      "epoch:[ 21/200] binary_loss:0.576849102973938 triangle_loss:0.6919821500778198 origin_loss:4.236663341522217\n",
      "epoch:[ 22/200] binary_loss:0.571341872215271 triangle_loss:0.6960058212280273 origin_loss:6.050136089324951\n",
      "epoch:[ 23/200] binary_loss:0.5722900629043579 triangle_loss:0.6991226077079773 origin_loss:6.566732406616211\n",
      "epoch:[ 24/200] binary_loss:0.5546126365661621 triangle_loss:0.7047006487846375 origin_loss:4.965843677520752\n",
      "epoch:[ 25/200] binary_loss:0.564728319644928 triangle_loss:0.6888614296913147 origin_loss:4.809279441833496\n",
      "epoch:[ 26/200] binary_loss:0.5243043303489685 triangle_loss:0.6944065093994141 origin_loss:1.078822374343872\n",
      "epoch:[ 27/200] binary_loss:0.5426250100135803 triangle_loss:0.6965641975402832 origin_loss:5.407868385314941\n",
      "epoch:[ 28/200] binary_loss:0.5398126244544983 triangle_loss:0.6974223852157593 origin_loss:4.551272392272949\n",
      "epoch:[ 29/200] binary_loss:0.5203456282615662 triangle_loss:0.6939593553543091 origin_loss:2.7113089561462402\n",
      "epoch:[ 30/200] binary_loss:0.5111867785453796 triangle_loss:0.6940088272094727 origin_loss:2.442976474761963\n",
      "epoch:[ 31/200] binary_loss:0.5241106748580933 triangle_loss:0.6938029527664185 origin_loss:3.008760452270508\n",
      "epoch:[ 32/200] binary_loss:0.5260311961174011 triangle_loss:0.6884706616401672 origin_loss:2.7661871910095215\n",
      "epoch:[ 33/200] binary_loss:0.5120359063148499 triangle_loss:0.6904221177101135 origin_loss:0.7845602035522461\n",
      "epoch:[ 34/200] binary_loss:0.48890921473503113 triangle_loss:0.6909964680671692 origin_loss:2.0516700744628906\n",
      "epoch:[ 35/200] binary_loss:0.4827214777469635 triangle_loss:0.6962442994117737 origin_loss:1.3384929895401\n",
      "epoch:[ 36/200] binary_loss:0.48885440826416016 triangle_loss:0.7017158269882202 origin_loss:2.092763900756836\n",
      "epoch:[ 37/200] binary_loss:0.4982365667819977 triangle_loss:0.6895340085029602 origin_loss:2.3815908432006836\n",
      "epoch:[ 38/200] binary_loss:0.4856633245944977 triangle_loss:0.6928762793540955 origin_loss:0.7130910754203796\n",
      "epoch:[ 39/200] binary_loss:0.48265963792800903 triangle_loss:0.6963449716567993 origin_loss:2.2626967430114746\n",
      "epoch:[ 40/200] binary_loss:0.454795777797699 triangle_loss:0.7114068269729614 origin_loss:2.2706518173217773\n",
      "epoch:[ 41/200] binary_loss:0.4513421952724457 triangle_loss:0.6965122222900391 origin_loss:1.0171977281570435\n",
      "epoch:[ 42/200] binary_loss:0.4239877164363861 triangle_loss:0.6824066042900085 origin_loss:3.3366150856018066\n",
      "epoch:[ 43/200] binary_loss:0.44531551003456116 triangle_loss:0.6963104605674744 origin_loss:2.4402527809143066\n",
      "epoch:[ 44/200] binary_loss:0.442457914352417 triangle_loss:0.6910649538040161 origin_loss:1.375940203666687\n",
      "epoch:[ 45/200] binary_loss:0.42885127663612366 triangle_loss:0.6793301105499268 origin_loss:1.1944801807403564\n",
      "epoch:[ 46/200] binary_loss:0.41018545627593994 triangle_loss:0.6838625073432922 origin_loss:2.426095485687256\n",
      "epoch:[ 47/200] binary_loss:0.4177663326263428 triangle_loss:0.7011746168136597 origin_loss:2.045269250869751\n",
      "epoch:[ 48/200] binary_loss:0.4092172682285309 triangle_loss:0.6895523071289062 origin_loss:0.7601649761199951\n",
      "epoch:[ 49/200] binary_loss:0.4036576449871063 triangle_loss:0.6964073181152344 origin_loss:3.113070249557495\n",
      "epoch:[ 50/200] binary_loss:0.4094778299331665 triangle_loss:0.7039382457733154 origin_loss:3.12691068649292\n",
      "epoch:[ 51/200] binary_loss:0.38065823912620544 triangle_loss:0.70387202501297 origin_loss:0.9295028448104858\n",
      "epoch:[ 52/200] binary_loss:0.38542598485946655 triangle_loss:0.689373791217804 origin_loss:2.0076301097869873\n",
      "epoch:[ 53/200] binary_loss:0.3746945261955261 triangle_loss:0.6892282962799072 origin_loss:1.2521824836730957\n",
      "epoch:[ 54/200] binary_loss:0.3558580279350281 triangle_loss:0.7045082449913025 origin_loss:1.554581642150879\n",
      "epoch:[ 55/200] binary_loss:0.3838288187980652 triangle_loss:0.6708875298500061 origin_loss:2.4315130710601807\n",
      "epoch:[ 56/200] binary_loss:0.3504653573036194 triangle_loss:0.6960404515266418 origin_loss:1.1471725702285767\n",
      "epoch:[ 57/200] binary_loss:0.35902509093284607 triangle_loss:0.6887332201004028 origin_loss:1.36702299118042\n",
      "epoch:[ 58/200] binary_loss:0.35262101888656616 triangle_loss:0.7100775837898254 origin_loss:1.0076611042022705\n",
      "epoch:[ 59/200] binary_loss:0.33532989025115967 triangle_loss:0.7020472884178162 origin_loss:2.4345083236694336\n",
      "epoch:[ 60/200] binary_loss:0.32175207138061523 triangle_loss:0.6896390914916992 origin_loss:2.259103775024414\n",
      "epoch:[ 61/200] binary_loss:0.31157442927360535 triangle_loss:0.6837162375450134 origin_loss:0.7770151495933533\n",
      "epoch:[ 62/200] binary_loss:0.3145330250263214 triangle_loss:0.7004241347312927 origin_loss:3.46669340133667\n",
      "epoch:[ 63/200] binary_loss:0.3079836070537567 triangle_loss:0.697003185749054 origin_loss:2.5996756553649902\n",
      "epoch:[ 64/200] binary_loss:0.3224138915538788 triangle_loss:0.6847421526908875 origin_loss:1.2387276887893677\n",
      "epoch:[ 65/200] binary_loss:0.304223895072937 triangle_loss:0.6960766315460205 origin_loss:2.0109214782714844\n",
      "epoch:[ 66/200] binary_loss:0.2903444170951843 triangle_loss:0.69034343957901 origin_loss:3.183044672012329\n",
      "epoch:[ 67/200] binary_loss:0.2922017276287079 triangle_loss:0.6900668144226074 origin_loss:0.8038842678070068\n",
      "epoch:[ 68/200] binary_loss:0.2764824330806732 triangle_loss:0.6822840571403503 origin_loss:1.5272748470306396\n",
      "epoch:[ 69/200] binary_loss:0.2803954482078552 triangle_loss:0.6820774674415588 origin_loss:2.048795461654663\n",
      "epoch:[ 70/200] binary_loss:0.27685508131980896 triangle_loss:0.6681204438209534 origin_loss:0.7206596732139587\n",
      "epoch:[ 71/200] binary_loss:0.2825746238231659 triangle_loss:0.6895530819892883 origin_loss:1.203648328781128\n",
      "epoch:[ 72/200] binary_loss:0.267378568649292 triangle_loss:0.6949990391731262 origin_loss:0.6912379860877991\n",
      "epoch:[ 73/200] binary_loss:0.2624054253101349 triangle_loss:0.7102674245834351 origin_loss:1.3324981927871704\n",
      "epoch:[ 74/200] binary_loss:0.2592426538467407 triangle_loss:0.6956061720848083 origin_loss:0.8078610301017761\n",
      "epoch:[ 75/200] binary_loss:0.2388114184141159 triangle_loss:0.7085279226303101 origin_loss:0.7872973084449768\n",
      "epoch:[ 76/200] binary_loss:0.2498103678226471 triangle_loss:0.7143706679344177 origin_loss:0.7895103096961975\n",
      "epoch:[ 77/200] binary_loss:0.2495998740196228 triangle_loss:0.6946998834609985 origin_loss:0.7280941605567932\n",
      "epoch:[ 78/200] binary_loss:0.2221311777830124 triangle_loss:0.7052234411239624 origin_loss:1.0244276523590088\n",
      "epoch:[ 79/200] binary_loss:0.2232351005077362 triangle_loss:0.703346312046051 origin_loss:0.7086902260780334\n",
      "epoch:[ 80/200] binary_loss:0.2105594277381897 triangle_loss:0.6976587176322937 origin_loss:0.7661758065223694\n",
      "epoch:[ 81/200] binary_loss:0.2023281455039978 triangle_loss:0.6943191289901733 origin_loss:0.9605141878128052\n",
      "epoch:[ 82/200] binary_loss:0.22747276723384857 triangle_loss:0.6962918639183044 origin_loss:0.6980372667312622\n",
      "epoch:[ 83/200] binary_loss:0.20634843409061432 triangle_loss:0.6876935958862305 origin_loss:0.8451524972915649\n",
      "epoch:[ 84/200] binary_loss:0.19815467298030853 triangle_loss:0.6919587254524231 origin_loss:0.8076283931732178\n",
      "epoch:[ 85/200] binary_loss:0.2119293510913849 triangle_loss:0.695551335811615 origin_loss:0.6835743188858032\n",
      "epoch:[ 86/200] binary_loss:0.18542352318763733 triangle_loss:0.6960914134979248 origin_loss:0.7016379237174988\n",
      "epoch:[ 87/200] binary_loss:0.20857815444469452 triangle_loss:0.688342809677124 origin_loss:0.7000477910041809\n",
      "epoch:[ 88/200] binary_loss:0.18668517470359802 triangle_loss:0.6933719515800476 origin_loss:0.6942405104637146\n",
      "epoch:[ 89/200] binary_loss:0.17689242959022522 triangle_loss:0.6969907879829407 origin_loss:0.725574254989624\n",
      "epoch:[ 90/200] binary_loss:0.1765035092830658 triangle_loss:0.6943527460098267 origin_loss:0.7483500242233276\n",
      "epoch:[ 91/200] binary_loss:0.16177265346050262 triangle_loss:0.6898742914199829 origin_loss:0.7039273977279663\n",
      "epoch:[ 92/200] binary_loss:0.1863662153482437 triangle_loss:0.6932089328765869 origin_loss:0.6911659836769104\n",
      "epoch:[ 93/200] binary_loss:0.17570847272872925 triangle_loss:0.6944540739059448 origin_loss:0.7154741287231445\n",
      "epoch:[ 94/200] binary_loss:0.16655488312244415 triangle_loss:0.6921955943107605 origin_loss:0.7207168340682983\n",
      "epoch:[ 95/200] binary_loss:0.16621196269989014 triangle_loss:0.6977362036705017 origin_loss:0.695478618144989\n",
      "epoch:[ 96/200] binary_loss:0.15088799595832825 triangle_loss:0.6874958872795105 origin_loss:0.6733831167221069\n",
      "epoch:[ 97/200] binary_loss:0.14286692440509796 triangle_loss:0.6912994384765625 origin_loss:0.8340612053871155\n",
      "epoch:[ 98/200] binary_loss:0.14626899361610413 triangle_loss:0.6969191431999207 origin_loss:0.951200008392334\n",
      "epoch:[ 99/200] binary_loss:0.13898897171020508 triangle_loss:0.6898265480995178 origin_loss:0.7024137377738953\n",
      "epoch:[100/200] binary_loss:0.12772198021411896 triangle_loss:0.6935615539550781 origin_loss:0.8891034722328186\n",
      "epoch:[101/200] binary_loss:0.13615547120571136 triangle_loss:0.6902524828910828 origin_loss:0.876529335975647\n",
      "epoch:[102/200] binary_loss:0.1368696093559265 triangle_loss:0.6901503801345825 origin_loss:1.7799768447875977\n",
      "epoch:[103/200] binary_loss:0.11514176428318024 triangle_loss:0.6901337504386902 origin_loss:1.1336190700531006\n",
      "epoch:[104/200] binary_loss:0.14130844175815582 triangle_loss:0.687634289264679 origin_loss:2.2398648262023926\n",
      "epoch:[105/200] binary_loss:0.12069921940565109 triangle_loss:0.692103385925293 origin_loss:2.6389667987823486\n",
      "epoch:[106/200] binary_loss:0.11108091473579407 triangle_loss:0.6862013936042786 origin_loss:1.5090280771255493\n",
      "epoch:[107/200] binary_loss:0.11332057416439056 triangle_loss:0.6919528245925903 origin_loss:2.4566643238067627\n",
      "epoch:[108/200] binary_loss:0.10871178656816483 triangle_loss:0.687377393245697 origin_loss:5.361363410949707\n",
      "epoch:[109/200] binary_loss:0.1323341578245163 triangle_loss:0.6944746375083923 origin_loss:3.039844036102295\n",
      "epoch:[110/200] binary_loss:0.095506452023983 triangle_loss:0.7080479860305786 origin_loss:1.127760648727417\n",
      "epoch:[111/200] binary_loss:0.11259392648935318 triangle_loss:0.6918506622314453 origin_loss:2.388592481613159\n",
      "epoch:[112/200] binary_loss:0.10051442682743073 triangle_loss:0.7013694047927856 origin_loss:0.7327517867088318\n",
      "epoch:[113/200] binary_loss:0.09991258382797241 triangle_loss:0.6949799060821533 origin_loss:1.8605810403823853\n",
      "epoch:[114/200] binary_loss:0.10168365389108658 triangle_loss:0.6933864951133728 origin_loss:2.4932992458343506\n",
      "epoch:[115/200] binary_loss:0.08752864599227905 triangle_loss:0.6823626756668091 origin_loss:0.7203071117401123\n",
      "epoch:[116/200] binary_loss:0.08649759739637375 triangle_loss:0.6858214735984802 origin_loss:2.1679165363311768\n",
      "epoch:[117/200] binary_loss:0.08971147239208221 triangle_loss:0.6870915293693542 origin_loss:3.2604293823242188\n",
      "epoch:[118/200] binary_loss:0.08288431912660599 triangle_loss:0.6923970580101013 origin_loss:0.7936244010925293\n",
      "epoch:[119/200] binary_loss:0.0842982828617096 triangle_loss:0.7073819041252136 origin_loss:2.997469663619995\n",
      "epoch:[120/200] binary_loss:0.09680286794900894 triangle_loss:0.696508526802063 origin_loss:3.780165433883667\n",
      "epoch:[121/200] binary_loss:0.08652789145708084 triangle_loss:0.6985299587249756 origin_loss:1.7138829231262207\n",
      "epoch:[122/200] binary_loss:0.07835863530635834 triangle_loss:0.6903572082519531 origin_loss:2.6759748458862305\n",
      "epoch:[123/200] binary_loss:0.08530818670988083 triangle_loss:0.6854627132415771 origin_loss:3.169684410095215\n",
      "epoch:[124/200] binary_loss:0.08764059841632843 triangle_loss:0.7012335062026978 origin_loss:1.042251706123352\n",
      "epoch:[125/200] binary_loss:0.08478764444589615 triangle_loss:0.7063227295875549 origin_loss:2.8906428813934326\n",
      "epoch:[126/200] binary_loss:0.07852668315172195 triangle_loss:0.7049480080604553 origin_loss:5.6561198234558105\n",
      "epoch:[127/200] binary_loss:0.0811678022146225 triangle_loss:0.6802017688751221 origin_loss:3.109095573425293\n",
      "epoch:[128/200] binary_loss:0.07246676087379456 triangle_loss:0.6935749650001526 origin_loss:0.7487837076187134\n",
      "epoch:[129/200] binary_loss:0.07044298201799393 triangle_loss:0.7022777199745178 origin_loss:3.2509870529174805\n",
      "epoch:[130/200] binary_loss:0.07041819393634796 triangle_loss:0.7126251459121704 origin_loss:3.229290008544922\n",
      "epoch:[131/200] binary_loss:0.06961921602487564 triangle_loss:0.691372811794281 origin_loss:5.487561225891113\n",
      "epoch:[132/200] binary_loss:0.06656275689601898 triangle_loss:0.6888192296028137 origin_loss:1.9402388334274292\n",
      "epoch:[133/200] binary_loss:0.0645320937037468 triangle_loss:0.68475741147995 origin_loss:3.5155093669891357\n",
      "epoch:[134/200] binary_loss:0.06718369573354721 triangle_loss:0.7006494402885437 origin_loss:7.323747158050537\n",
      "epoch:[135/200] binary_loss:0.06353143602609634 triangle_loss:0.6876631379127502 origin_loss:6.278201580047607\n",
      "epoch:[136/200] binary_loss:0.06759455800056458 triangle_loss:0.6961510181427002 origin_loss:5.258127212524414\n",
      "epoch:[137/200] binary_loss:0.060593247413635254 triangle_loss:0.6949125528335571 origin_loss:0.8664026856422424\n",
      "epoch:[138/200] binary_loss:0.06355620920658112 triangle_loss:0.6999309062957764 origin_loss:3.676255941390991\n",
      "epoch:[139/200] binary_loss:0.0517202764749527 triangle_loss:0.6957522630691528 origin_loss:6.86445951461792\n",
      "epoch:[140/200] binary_loss:0.05425957217812538 triangle_loss:0.6924275755882263 origin_loss:6.270211219787598\n",
      "epoch:[141/200] binary_loss:0.05130635201931 triangle_loss:0.6964737176895142 origin_loss:7.5930938720703125\n",
      "epoch:[142/200] binary_loss:0.040976837277412415 triangle_loss:0.6947215795516968 origin_loss:3.7717576026916504\n",
      "epoch:[143/200] binary_loss:0.06092624366283417 triangle_loss:0.6899179816246033 origin_loss:3.885256052017212\n",
      "epoch:[144/200] binary_loss:0.05417744815349579 triangle_loss:0.6956518888473511 origin_loss:6.35058069229126\n",
      "epoch:[145/200] binary_loss:0.04716465622186661 triangle_loss:0.6922026872634888 origin_loss:8.708785057067871\n",
      "epoch:[146/200] binary_loss:0.03843593969941139 triangle_loss:0.6927590370178223 origin_loss:5.383594512939453\n",
      "epoch:[147/200] binary_loss:0.05326106771826744 triangle_loss:0.6861076951026917 origin_loss:2.6733367443084717\n",
      "epoch:[148/200] binary_loss:0.06043548136949539 triangle_loss:0.6890475153923035 origin_loss:2.419140577316284\n",
      "epoch:[149/200] binary_loss:0.04881884902715683 triangle_loss:0.6953604817390442 origin_loss:3.814293622970581\n",
      "epoch:[150/200] binary_loss:0.04605145752429962 triangle_loss:0.6958069205284119 origin_loss:3.042513132095337\n",
      "epoch:[151/200] binary_loss:0.041860319674015045 triangle_loss:0.696122944355011 origin_loss:0.7048391699790955\n",
      "epoch:[152/200] binary_loss:0.037437401711940765 triangle_loss:0.6884347796440125 origin_loss:2.256472587585449\n",
      "epoch:[153/200] binary_loss:0.0400431789457798 triangle_loss:0.6935689449310303 origin_loss:4.893984317779541\n",
      "epoch:[154/200] binary_loss:0.03959518298506737 triangle_loss:0.6866165995597839 origin_loss:1.3660632371902466\n",
      "epoch:[155/200] binary_loss:0.052060458809137344 triangle_loss:0.7079979777336121 origin_loss:1.760554552078247\n",
      "epoch:[156/200] binary_loss:0.037945687770843506 triangle_loss:0.6789488196372986 origin_loss:5.268474578857422\n",
      "epoch:[157/200] binary_loss:0.041535403579473495 triangle_loss:0.6868088841438293 origin_loss:3.6021039485931396\n",
      "epoch:[158/200] binary_loss:0.04050735756754875 triangle_loss:0.6895782351493835 origin_loss:0.789107620716095\n",
      "epoch:[159/200] binary_loss:0.038463253527879715 triangle_loss:0.693579375743866 origin_loss:3.370272397994995\n",
      "epoch:[160/200] binary_loss:0.03212359547615051 triangle_loss:0.6959198117256165 origin_loss:1.5537272691726685\n",
      "epoch:[161/200] binary_loss:0.037256937474012375 triangle_loss:0.7071434855461121 origin_loss:1.950494647026062\n",
      "epoch:[162/200] binary_loss:0.03797942027449608 triangle_loss:0.6938918828964233 origin_loss:3.242302656173706\n",
      "epoch:[163/200] binary_loss:0.041252825409173965 triangle_loss:0.7100152969360352 origin_loss:0.6823515892028809\n",
      "epoch:[164/200] binary_loss:0.03435976058244705 triangle_loss:0.6873098015785217 origin_loss:1.7277344465255737\n",
      "epoch:[165/200] binary_loss:0.032651349902153015 triangle_loss:0.6913737058639526 origin_loss:1.5551472902297974\n",
      "epoch:[166/200] binary_loss:0.03461290895938873 triangle_loss:0.6935893297195435 origin_loss:1.4802002906799316\n",
      "epoch:[167/200] binary_loss:0.04245539754629135 triangle_loss:0.6951685547828674 origin_loss:1.8507494926452637\n",
      "epoch:[168/200] binary_loss:0.03427766636013985 triangle_loss:0.6920654773712158 origin_loss:1.2124189138412476\n",
      "epoch:[169/200] binary_loss:0.03074006922543049 triangle_loss:0.6985407471656799 origin_loss:3.914273977279663\n",
      "epoch:[170/200] binary_loss:0.03159628435969353 triangle_loss:0.6887461543083191 origin_loss:2.1902830600738525\n",
      "epoch:[171/200] binary_loss:0.028655314818024635 triangle_loss:0.6959819793701172 origin_loss:0.6954247951507568\n",
      "epoch:[172/200] binary_loss:0.03784453496336937 triangle_loss:0.6983382701873779 origin_loss:2.2794580459594727\n",
      "epoch:[173/200] binary_loss:0.030401241034269333 triangle_loss:0.6895791888237 origin_loss:2.2745752334594727\n",
      "epoch:[174/200] binary_loss:0.02523604966700077 triangle_loss:0.6969538927078247 origin_loss:0.7948993444442749\n",
      "epoch:[175/200] binary_loss:0.02768283523619175 triangle_loss:0.6914940476417542 origin_loss:2.5793118476867676\n",
      "epoch:[176/200] binary_loss:0.027413194999098778 triangle_loss:0.7008165121078491 origin_loss:3.107913017272949\n",
      "epoch:[177/200] binary_loss:0.025515709072351456 triangle_loss:0.6954799890518188 origin_loss:1.4279037714004517\n",
      "epoch:[178/200] binary_loss:0.028135864064097404 triangle_loss:0.6901707649230957 origin_loss:2.110584259033203\n",
      "epoch:[179/200] binary_loss:0.03260519728064537 triangle_loss:0.688378095626831 origin_loss:3.9851977825164795\n",
      "epoch:[180/200] binary_loss:0.024494122713804245 triangle_loss:0.694216251373291 origin_loss:3.6094977855682373\n",
      "epoch:[181/200] binary_loss:0.024996260181069374 triangle_loss:0.6849022507667542 origin_loss:0.7197354435920715\n",
      "epoch:[182/200] binary_loss:0.025226108729839325 triangle_loss:0.6919670104980469 origin_loss:2.90372371673584\n",
      "epoch:[183/200] binary_loss:0.030825594440102577 triangle_loss:0.697807788848877 origin_loss:2.219836711883545\n",
      "epoch:[184/200] binary_loss:0.02299787849187851 triangle_loss:0.6935791969299316 origin_loss:0.9706622958183289\n",
      "epoch:[185/200] binary_loss:0.022575298324227333 triangle_loss:0.6921379566192627 origin_loss:3.2727930545806885\n",
      "epoch:[186/200] binary_loss:0.024245237931609154 triangle_loss:0.6936569213867188 origin_loss:2.564678192138672\n",
      "epoch:[187/200] binary_loss:0.02511601336300373 triangle_loss:0.6916142106056213 origin_loss:1.6069061756134033\n",
      "epoch:[188/200] binary_loss:0.02285928465425968 triangle_loss:0.6886249780654907 origin_loss:1.2139039039611816\n",
      "epoch:[189/200] binary_loss:0.026819679886102676 triangle_loss:0.6917797923088074 origin_loss:2.4676527976989746\n",
      "epoch:[190/200] binary_loss:0.022400056943297386 triangle_loss:0.6969032883644104 origin_loss:1.6492072343826294\n",
      "epoch:[191/200] binary_loss:0.02470509707927704 triangle_loss:0.6954671144485474 origin_loss:3.0599381923675537\n",
      "epoch:[192/200] binary_loss:0.0231486689299345 triangle_loss:0.6977156400680542 origin_loss:4.135528564453125\n",
      "epoch:[193/200] binary_loss:0.024425290524959564 triangle_loss:0.6975620985031128 origin_loss:3.454710006713867\n",
      "epoch:[194/200] binary_loss:0.023887356743216515 triangle_loss:0.6981842517852783 origin_loss:1.3243358135223389\n",
      "epoch:[195/200] binary_loss:0.018893973901867867 triangle_loss:0.6980413198471069 origin_loss:2.4780077934265137\n",
      "epoch:[196/200] binary_loss:0.02293287217617035 triangle_loss:0.6993746757507324 origin_loss:2.5406863689422607\n",
      "epoch:[197/200] binary_loss:0.020013099536299706 triangle_loss:0.6974712610244751 origin_loss:0.8364153504371643\n",
      "epoch:[198/200] binary_loss:0.01902010291814804 triangle_loss:0.6927831172943115 origin_loss:3.662505626678467\n",
      "epoch:[199/200] binary_loss:0.02204802818596363 triangle_loss:0.6838313937187195 origin_loss:3.501659870147705\n",
      "----------------------------- Test -----------------------------\n",
      "binary_result:    tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "triangle_result:  tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
      "origin_result:    tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
      "label:            tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "### 结果\n",
    "\n",
    "![](demo/homework3_2_result.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设计\n",
    "\n",
    "这里为了网络参数公平，另编码长度保证相同，三种方式如下\n",
    "\n",
    "- 二进制方法：对 [0,999] 范围的数转变成 10 位二进制数\n",
    "- 三角函数法：对 sin、cos 进行交替编码，分别尝试了 postion_encoding、直接对原来的数进行 cos、sin\n",
    "- 直接数字法：将输入数字重复 10 次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析\n",
    "针对三种方案，其中 `二进制` 的方式是可行的\n",
    "\n",
    "- `二进制`方法：因为二进制最后一位数与数的奇偶性具有强相关性，故经过迭代基本能够无误实现判断\n",
    "\n",
    "- `三角函数`方法：按道理来说，三角函数具有周期性，只要能够学到周期为1，那应该是能够收敛的，但是我在测试的时候并没有收敛，可能是我设计的编码他的周期不为1，导致出现了奇偶模糊，因为 position_encoding 中有 $PE_{t+\\Delta t} = T_{\\Delta t} PE_{t}$ 的变换关系\n",
    "\n",
    "- `直接数字`方法：完全不收敛\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 题目2（应用实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo/homework3_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
